{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d08aaad-3454-4f10-8e37-36a67444de70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "\n",
    "from loguru import logger\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def load_data_in_batches(dataset_path, batch_size, split=-1):\n",
    "    \"\"\"\n",
    "    Generator function that reads data from a compressed file and yields batches of data.\n",
    "    Each batch is a dictionary containing lists of interaction_ids, queries, search results, query times, and answers.\n",
    "\n",
    "    Args:\n",
    "    dataset_path (str): Path to the dataset file.\n",
    "    batch_size (int): Number of data items in each batch.\n",
    "\n",
    "    Yields:\n",
    "    dict: A batch of data.\n",
    "    \"\"\"\n",
    "\n",
    "    def initialize_batch():\n",
    "        \"\"\" Helper function to create an empty batch. \"\"\"\n",
    "        return {\"interaction_id\": [], \"query\": [], \"search_results\": [], \"query_time\": [], \"answer\": [], \"domain\": [], \"static_or_dynamic\": [], \"question_type\": []}\n",
    "\n",
    "    try:\n",
    "        with bz2.open(dataset_path, \"rt\") as file:\n",
    "            batch = initialize_batch()\n",
    "            for line in file:\n",
    "                try:\n",
    "                    item = json.loads(line)\n",
    "\n",
    "                    if split != -1 and item[\"split\"] != split:\n",
    "                        continue\n",
    "\n",
    "                    for key in batch:\n",
    "                        batch[key].append(item[key])\n",
    "\n",
    "                    if len(batch[\"query\"]) == batch_size:\n",
    "                        yield batch\n",
    "                        batch = initialize_batch()\n",
    "                except json.JSONDecodeError:\n",
    "                    logger.warn(\"Warning: Failed to decode a line.\")\n",
    "            # Yield any remaining data as the last batch\n",
    "            if batch[\"query\"]:\n",
    "                yield batch\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Error: The file {dataset_path} was not found.\")\n",
    "        raise e\n",
    "    except IOError as e:\n",
    "        logger.error(f\"Error: An error occurred while reading the file {dataset_path}.\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "def generate_predictions(dataset_path, model, split):\n",
    "    \"\"\"\n",
    "    Processes batches of data from a dataset to generate predictions using a model.\n",
    "\n",
    "    Args:\n",
    "    dataset_path (str): Path to the dataset.\n",
    "    model (object): UserModel that provides `get_batch_size()` and `batch_generate_answer()` interfaces.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing lists of queries, ground truths, and predictions.\n",
    "    \"\"\"\n",
    "    queries, ground_truths, predictions = [], [], []\n",
    "    batch_size = model.get_batch_size()\n",
    "\n",
    "    for batch in tqdm(load_data_in_batches(dataset_path, batch_size, split), desc=\"Generating predictions\"):\n",
    "        batch_ground_truths = batch.pop(\"answer\")  # Remove answers from batch and store them\n",
    "        # batch_predictions = model.batch_generate_answer(batch)\n",
    "\n",
    "        queries.extend(batch[\"query\"])\n",
    "        ground_truths.extend(batch_ground_truths)\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "    return queries, ground_truths, predictions\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--dataset_path\", type=str, default=\"example_data/dev_data.jsonl.bz2\",\n",
    "                        choices=[\"example_data/dev_data.jsonl.bz2\", # example data\n",
    "                                 \"data/crag_task_1_dev_v4_release.jsonl.bz2\", # full data\n",
    "                                 ])\n",
    "    parser.add_argument(\"--split\", type=int, default=-1,\n",
    "                        help=\"The split of the dataset to use. This is only relevant for the full data: \"\n",
    "                             \"0 for public validation set, 1 for public test set\")\n",
    "\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"vanilla_baseline\",\n",
    "                        choices=[\"vanilla_baseline\",\n",
    "                                 \"rag_baseline\"\n",
    "                                 # add your model here\n",
    "                                 ],\n",
    "                        )\n",
    "\n",
    "    parser.add_argument(\"--llm_name\", type=str, default=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "                        choices=[\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "                                 \"google/gemma-2-2b-it\",\n",
    "                                 # can add more llm models here\n",
    "                                 ])\n",
    "    parser.add_argument(\"--is_server\", action=\"store_true\", default=False,\n",
    "                        help=\"Whether we use vLLM deployed on a server or offline inference.\")\n",
    "    parser.add_argument(\"--vllm_server\", type=str, default=\"http://localhost:8088/v1\",\n",
    "                        help=\"URL of the vLLM server if is_server is True. The port number may vary.\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    print(args.is_server)\n",
    "\n",
    "    dataset_path = args.dataset_path\n",
    "    dataset = dataset_path.split(\"/\")[0]\n",
    "    split = -1\n",
    "    if dataset == \"data\":\n",
    "        split = args.split\n",
    "        if split == -1:\n",
    "            raise ValueError(\"Please provide a valid split value for the full data: \"\n",
    "                             \"0 for public validation set, 1 for public test set.\")\n",
    "    dataset_path = os.path.join(\"..\", dataset_path)\n",
    "\n",
    "    llm_name = args.llm_name\n",
    "    _llm_name = llm_name.split(\"/\")[-1]\n",
    "    \n",
    "    model_name = args.model_name\n",
    "    if model_name == \"vanilla_baseline\":\n",
    "        from vanilla_baseline import InstructModel\n",
    "        model = InstructModel(llm_name=llm_name, is_server=args.is_server, vllm_server=args.vllm_server)\n",
    "    elif model_name == \"rag_baseline\":\n",
    "        from rag_baseline import RAGModel\n",
    "        model = RAGModel(llm_name=llm_name, is_server=args.is_server, vllm_server=args.vllm_server)\n",
    "    # elif model_name == \"your_model\":\n",
    "    #     add your model here\n",
    "    else:\n",
    "        raise ValueError(\"Model name not recognized.\")\n",
    "\n",
    "    # make output directory\n",
    "    output_directory = os.path.join(\"..\", \"output\", dataset, model_name, _llm_name)\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Generate predictions\n",
    "    queries, ground_truths, predictions = generate_predictions(dataset_path, model, split)\n",
    "\n",
    "    # save predictions\n",
    "    json.dump({\"queries\": queries, \"ground_truths\": ground_truths, \"predictions\": predictions},\n",
    "              open(os.path.join(output_directory, \"predictions.json\"), \"w\"), indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337d0e1d-4be3-44b1-b7df-140750d16161",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
