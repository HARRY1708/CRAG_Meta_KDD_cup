{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d08aaad-3454-4f10-8e37-36a67444de70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "\n",
    "from loguru import logger\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def load_data_in_batches(dataset_path, batch_size, split=-1):\n",
    "    \"\"\"\n",
    "    Generator function that reads data from a compressed file and yields batches of data.\n",
    "    Each batch is a dictionary containing lists of interaction_ids, queries, search results, query times, and answers.\n",
    "\n",
    "    Args:\n",
    "    dataset_path (str): Path to the dataset file.\n",
    "    batch_size (int): Number of data items in each batch.\n",
    "\n",
    "    Yields:\n",
    "    dict: A batch of data.\n",
    "    \"\"\"\n",
    "\n",
    "    def initialize_batch():\n",
    "        \"\"\" Helper function to create an empty batch. \"\"\"\n",
    "        return {\"interaction_id\": [], \"query\": [], \"search_results\": [], \"query_time\": [], \"answer\": [], \"domain\": [], \"static_or_dynamic\": [], \"question_type\": []}\n",
    "\n",
    "    try:\n",
    "        with bz2.open(dataset_path, \"rt\") as file:\n",
    "            batch = initialize_batch()\n",
    "            for line in file:\n",
    "                try:\n",
    "                    item = json.loads(line)\n",
    "\n",
    "                    if split != -1 and item[\"split\"] != split:\n",
    "                        continue\n",
    "\n",
    "                    for key in batch:\n",
    "                        batch[key].append(item[key])\n",
    "\n",
    "                    if len(batch[\"query\"]) == batch_size:\n",
    "                        yield batch\n",
    "                        batch = initialize_batch()\n",
    "                except json.JSONDecodeError:\n",
    "                    logger.warn(\"Warning: Failed to decode a line.\")\n",
    "            # Yield any remaining data as the last batch\n",
    "            if batch[\"query\"]:\n",
    "                yield batch\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Error: The file {dataset_path} was not found.\")\n",
    "        raise e\n",
    "    except IOError as e:\n",
    "        logger.error(f\"Error: An error occurred while reading the file {dataset_path}.\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "def generate_predictions(dataset_path, model, split):\n",
    "    \"\"\"\n",
    "    Processes batches of data from a dataset to generate predictions using a model.\n",
    "\n",
    "    Args:\n",
    "    dataset_path (str): Path to the dataset.\n",
    "    model (object): UserModel that provides `get_batch_size()` and `batch_generate_answer()` interfaces.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing lists of queries, ground truths, and predictions.\n",
    "    \"\"\"\n",
    "    queries, ground_truths, predictions = [], [], []\n",
    "    batch_size = model.get_batch_size()\n",
    "\n",
    "    for batch in tqdm(load_data_in_batches(dataset_path, batch_size, split), desc=\"Generating predictions\"):\n",
    "        print(f\"batch : {batch}\")\n",
    "        break\n",
    "        batch_ground_truths = batch.pop(\"answer\")  # Remove answers from batch and store them\n",
    "        # batch_predictions = model.batch_generate_answer(batch)\n",
    "\n",
    "        queries.extend(batch[\"query\"])\n",
    "        ground_truths.extend(batch_ground_truths)\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "    return queries, ground_truths, predictions\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--dataset_path\", type=str, default=\"example_data/dev_data.jsonl.bz2\",\n",
    "                        choices=[\"example_data/dev_data.jsonl.bz2\", # example data\n",
    "                                 \"data/crag_task_1_dev_v4_release.jsonl.bz2\", # full data\n",
    "                                 ])\n",
    "    parser.add_argument(\"--split\", type=int, default=-1,\n",
    "                        help=\"The split of the dataset to use. This is only relevant for the full data: \"\n",
    "                             \"0 for public validation set, 1 for public test set\")\n",
    "\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"vanilla_baseline\",\n",
    "                        choices=[\"vanilla_baseline\",\n",
    "                                 \"rag_baseline\"\n",
    "                                 # add your model here\n",
    "                                 ],\n",
    "                        )\n",
    "\n",
    "    parser.add_argument(\"--llm_name\", type=str, default=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "                        choices=[\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "                                 \"google/gemma-2-2b-it\",\n",
    "                                 # can add more llm models here\n",
    "                                 ])\n",
    "    parser.add_argument(\"--is_server\", action=\"store_true\", default=False,\n",
    "                        help=\"Whether we use vLLM deployed on a server or offline inference.\")\n",
    "    parser.add_argument(\"--vllm_server\", type=str, default=\"http://localhost:8088/v1\",\n",
    "                        help=\"URL of the vLLM server if is_server is True. The port number may vary.\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    print(args.is_server)\n",
    "\n",
    "    dataset_path = args.dataset_path\n",
    "    dataset = dataset_path.split(\"/\")[0]\n",
    "    split = -1\n",
    "    if dataset == \"data\":\n",
    "        split = args.split\n",
    "        if split == -1:\n",
    "            raise ValueError(\"Please provide a valid split value for the full data: \"\n",
    "                             \"0 for public validation set, 1 for public test set.\")\n",
    "    dataset_path = os.path.join(\"..\", dataset_path)\n",
    "\n",
    "    llm_name = args.llm_name\n",
    "    _llm_name = llm_name.split(\"/\")[-1]\n",
    "    \n",
    "    model_name = args.model_name\n",
    "    if model_name == \"vanilla_baseline\":\n",
    "        from vanilla_baseline import InstructModel\n",
    "        model = InstructModel(llm_name=llm_name, is_server=args.is_server, vllm_server=args.vllm_server)\n",
    "    elif model_name == \"rag_baseline\":\n",
    "        from rag_baseline import RAGModel\n",
    "        model = RAGModel(llm_name=llm_name, is_server=args.is_server, vllm_server=args.vllm_server)\n",
    "    # elif model_name == \"your_model\":\n",
    "    #     add your model here\n",
    "    else:\n",
    "        raise ValueError(\"Model name not recognized.\")\n",
    "\n",
    "    # make output directory\n",
    "    output_directory = os.path.join(\"..\", \"output\", dataset, model_name, _llm_name)\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Generate predictions\n",
    "    queries, ground_truths, predictions = generate_predictions(dataset_path, model, split)\n",
    "\n",
    "    # # save predictions\n",
    "    # json.dump({\"queries\": queries, \"ground_truths\": ground_truths, \"predictions\": predictions},\n",
    "    #           open(os.path.join(output_directory, \"predictions.json\"), \"w\"), indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7ea92596-48ae-49ee-bcb8-eaffe79c6e95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "dataset_path = '/home/jupyter/cs245-project-crag-master/data/crag_task_1_dev_v4_release.jsonl.bz2'\n",
    "with bz2.open(dataset_path, \"rt\") as file:\n",
    "    for line in file:\n",
    "        item = json.loads(line)\n",
    "        print(item['question_type'])\n",
    "        print(item['query_time'])\n",
    "        print(item['query'])\n",
    "        print(item['search_results'][0].keys())\n",
    "        for page in item['search_results']:\n",
    "            print(page['page_url'])\n",
    "            soup = BeautifulSoup(page['page_result'])\n",
    "            text = soup.get_text(\" \", strip=True)  # Use space as a separator, strip whitespaces\n",
    "            print(text)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "337d0e1d-4be3-44b1-b7df-140750d16161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_query_types(file_path,eval_type):\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    # Load JSON file\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    print(data.keys())\n",
    "    # Extract question type data\n",
    "    question_types = data[eval_type][eval_type]\n",
    "    extracted_data = []\n",
    "    for q_type, metrics in question_types.items():\n",
    "        eval_results = metrics[\"evaluation_results\"]\n",
    "        extracted_data.append([\n",
    "            q_type,\n",
    "            eval_results[\"score\"],\n",
    "            eval_results[\"exact_accuracy\"],\n",
    "            eval_results[\"accuracy\"],\n",
    "            eval_results[\"hallucination\"],\n",
    "            eval_results[\"missing\"],\n",
    "            eval_results[\"n_miss\"],\n",
    "            eval_results[\"n_correct\"],\n",
    "            eval_results[\"n_correct_exact\"],\n",
    "            eval_results[\"total\"]\n",
    "        ])\n",
    "\n",
    "    # Create DataFrame\n",
    "    columns = [\n",
    "        eval_type, \"Score\", \"Exact Accuracy\", \"Accuracy\", \"Hallucination\",\n",
    "        \"Missing\", \"N Miss\", \"N Correct\", \"N Correct Exact\", \"Total\"\n",
    "    ]\n",
    "    df = pd.DataFrame(extracted_data, columns=columns)\n",
    "\n",
    "    # Convert certain columns to percentages for better readability\n",
    "    df[[\"Exact Accuracy\", \"Accuracy\", \"Hallucination\", \"Missing\"]] *= 100\n",
    "\n",
    "    # Print the table\n",
    "    print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "709c1307-0785-4691-a326-f81168572135",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Question Type     Score  Exact Accuracy  Accuracy  Hallucination   Missing  N Miss  N Correct  N Correct Exact  Total\n",
      "            simple -0.041783       11.142061 28.133705      32.311978 39.554318     142        101               40    359\n",
      "simple_w_condition -0.116505        2.912621 25.242718      36.893204 37.864078      78         52                6    206\n",
      "        comparison  0.024540        0.613497 24.539877      22.085890 53.374233      87         40                1    163\n",
      "       aggregation -0.099379        4.347826 23.602484      33.540373 42.857143      69         38                7    161\n",
      "               set  0.200000        1.600000 49.600000      29.600000 20.800000      26         62                2    125\n",
      "     false_premise -0.039216        0.000000 13.725490      17.647059 68.627451     105         21                0    153\n",
      "   post-processing -0.295455        0.000000 13.636364      43.181818 43.181818      19          6                0     44\n",
      "         multi-hop -0.056452        3.225806 22.580645      28.225806 49.193548      61         28                4    124\n"
     ]
    }
   ],
   "source": [
    "# Specify the file path and call the function\n",
    "file_path = '/home/jupyter/cs245-project-crag-master/output/data/modified_rag/Llama-3.2-3B-Instruct/detailed_evaluation_results.json'\n",
    "analyze_query_types(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06e1989c-24c1-43c2-b58d-30349f910273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Question Type     Score  Exact Accuracy  Accuracy  Hallucination   Missing  N Miss  N Correct  N Correct Exact  Total\n",
      "            simple -0.089136        6.685237 21.448468      30.362117 48.189415     173         77               24    359\n",
      "simple_w_condition -0.087379        2.912621 23.300971      32.038835 44.660194      92         48                6    206\n",
      "        comparison -0.085890        1.226994 17.177914      25.766871 57.055215      93         28                2    163\n",
      "       aggregation -0.037267        3.105590 23.602484      27.329193 49.068323      79         38                5    161\n",
      "               set  0.016000        0.800000 39.200000      37.600000 23.200000      29         49                1    125\n",
      "     false_premise -0.013072        0.000000 14.379085      15.686275 69.934641     107         22                0    153\n",
      "   post-processing  0.000000        2.272727 29.545455      29.545455 40.909091      18         13                1     44\n",
      "         multi-hop -0.137097        3.225806 19.354839      33.064516 47.580645      59         24                4    124\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/jupyter/cs245-project-crag-master/output/data/rag_baseline/Llama-3.2-3B-Instruct/detailed_evaluation_results.json'\n",
    "analyze_query_types(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1493436-1842-4a4f-bbb9-897624f9566b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Question Type     Score  Exact Accuracy  Accuracy  Hallucination   Missing  N Miss  N Correct  N Correct Exact  Total\n",
      "            simple -0.100279       10.863510 25.069638      35.097493 39.832869     143         90               39    359\n",
      "simple_w_condition -0.029126        4.368932 23.300971      26.213592 50.485437     104         48                9    206\n",
      "        comparison -0.092025        1.226994 13.496933      22.699387 63.803681     104         22                2    163\n",
      "       aggregation  0.055901        3.105590 27.329193      21.739130 50.931677      82         44                5    161\n",
      "               set  0.192000        0.000000 44.800000      25.600000 29.600000      37         56                0    125\n",
      "     false_premise  0.071895        0.000000 13.725490       6.535948 79.738562     122         21                0    153\n",
      "   post-processing  0.090909        2.272727 22.727273      13.636364 63.636364      28         10                1     44\n",
      "         multi-hop  0.024194        4.032258 25.806452      23.387097 50.806452      63         32                5    124\n"
     ]
    }
   ],
   "source": [
    "# Specify the file path and call the function\n",
    "file_path = '/home/jupyter/cs245-project-crag-master/output/data/modified_rag/Llama-3.2-3B-Instruct/detailed_evaluation_results.json'\n",
    "analyze_query_types(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b0cc9909-9c86-42d6-b607-e4b0b168c3db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['domain', 'static_or_dynamic', 'question_type'])\n",
      "     question_type     Score  Exact Accuracy  Accuracy  Hallucination   Missing  N Miss  N Correct  N Correct Exact  Total\n",
      "            simple -0.061281        1.671309 35.376045      41.504178 23.119777      83        127                6    359\n",
      "simple_w_condition  0.067961        1.941748 42.233010      35.436893 22.330097      46         87                4    206\n",
      "        comparison  0.067485        6.134969 38.650307      31.901840 29.447853      48         63               10    163\n",
      "       aggregation -0.186335        1.863354 29.192547      47.826087 22.981366      37         47                3    161\n",
      "               set  0.096000        0.800000 49.600000      40.000000 10.400000      13         62                1    125\n",
      "     false_premise  0.261438        0.000000 49.673203      23.529412 26.797386      41         76                0    153\n",
      "   post-processing -0.090909        9.090909 36.363636      45.454545 18.181818       8         16                4     44\n",
      "         multi-hop -0.016129        4.838710 37.096774      38.709677 24.193548      30         46                6    124\n",
      "dict_keys(['domain', 'static_or_dynamic', 'question_type'])\n",
      "static_or_dynamic     Score  Exact Accuracy  Accuracy  Hallucination   Missing  N Miss  N Correct  N Correct Exact  Total\n",
      "           static  0.048193        2.811245 42.838019      38.018742 19.143240     143        320               21    747\n",
      "    slow-changing  0.006873        2.405498 42.955326      42.268041 14.776632      43        125                7    291\n",
      "    fast-changing -0.101695        2.259887 26.553672      36.723164 36.723164      65         47                4    177\n",
      "        real-time -0.141667        1.666667 20.000000      34.166667 45.833333      55         24                2    120\n",
      "dict_keys(['domain', 'static_or_dynamic', 'question_type'])\n",
      " domain     Score  Exact Accuracy  Accuracy  Hallucination   Missing  N Miss  N Correct  N Correct Exact  Total\n",
      "finance -0.052147        1.533742 26.380368      31.595092 42.024540     137         86                5    326\n",
      "  music -0.033520        1.675978 41.899441      45.251397 12.849162      23         75                3    179\n",
      "  movie  0.090615        1.294498 47.896440      38.834951 13.268608      41        148                4    309\n",
      " sports -0.118577        3.557312 30.830040      42.687747 26.482213      67         78                9    253\n",
      "   open  0.097015        4.850746 47.761194      38.059701 14.179104      38        128               13    268\n"
     ]
    }
   ],
   "source": [
    "# GPT-4\n",
    "file_path = '/home/jupyter/cs245-project-crag-master/output/data/modified_rag/Llama-3.2-3B-Instruct/detailed_evaluation_results.json'\n",
    "analyze_query_types(file_path,'question_type')\n",
    "analyze_query_types(file_path,'static_or_dynamic')\n",
    "analyze_query_types(file_path,'domain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7f5a71aa-e3fa-44af-88bd-47c792d6b284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['domain', 'static_or_dynamic', 'question_type'])\n",
      "     question_type     Score  Exact Accuracy  Accuracy  Hallucination   Missing  N Miss  N Correct  N Correct Exact  Total\n",
      "            simple -0.089136        1.671309 33.983287      42.896936 23.119777      83        122                6    359\n",
      "simple_w_condition -0.106796        1.941748 33.495146      44.174757 22.330097      46         69                4    206\n",
      "        comparison -0.006135        6.134969 34.969325      35.582822 29.447853      48         57               10    163\n",
      "       aggregation -0.161491        1.863354 30.434783      46.583851 22.981366      37         49                3    161\n",
      "               set -0.032000        0.800000 43.200000      46.400000 10.400000      13         54                1    125\n",
      "     false_premise -0.718954        0.000000  0.653595      72.549020 26.797386      41          1                0    153\n",
      "   post-processing -0.272727        9.090909 27.272727      54.545455 18.181818       8         12                4     44\n",
      "         multi-hop  0.032258        4.838710 39.516129      36.290323 24.193548      30         49                6    124\n",
      "dict_keys(['domain', 'static_or_dynamic', 'question_type'])\n",
      "static_or_dynamic     Score  Exact Accuracy  Accuracy  Hallucination   Missing  N Miss  N Correct  N Correct Exact  Total\n",
      "           static -0.053548        2.811245 37.751004      43.105756 19.143240     143        282               21    747\n",
      "    slow-changing -0.164948        2.405498 34.364261      50.859107 14.776632      43        100                7    291\n",
      "    fast-changing -0.350282        2.259887 14.124294      49.152542 36.723164      65         25                4    177\n",
      "        real-time -0.341667        1.666667 10.000000      44.166667 45.833333      55         12                2    120\n",
      "dict_keys(['domain', 'static_or_dynamic', 'question_type'])\n",
      " domain     Score  Exact Accuracy  Accuracy  Hallucination   Missing  N Miss  N Correct  N Correct Exact  Total\n",
      "finance -0.334356        1.533742 12.269939      45.705521 42.024540     137         40                5    326\n",
      "  music -0.089385        1.675978 39.106145      48.044693 12.849162      23         70                3    179\n",
      "  movie -0.161812        1.294498 35.275081      51.456311 13.268608      41        109                4    309\n",
      " sports -0.134387        3.557312 30.039526      43.478261 26.482213      67         76                9    253\n",
      "   open  0.067164        4.850746 46.268657      39.552239 14.179104      38        124               13    268\n"
     ]
    }
   ],
   "source": [
    "# GPT-4\n",
    "file_path = '/home/jupyter/cs245-project-crag-master/output/data/modified_rag/gpt-4/detailed_evaluation_results.json'\n",
    "analyze_query_types(file_path,'question_type')\n",
    "analyze_query_types(file_path,'static_or_dynamic')\n",
    "analyze_query_types(file_path,'domain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bb703780-7ff6-4868-9857-5356414682eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['domain', 'static_or_dynamic', 'question_type'])\n",
      "     question_type     Score  Exact Accuracy  Accuracy  Hallucination  Missing  N Miss  N Correct  N Correct Exact  Total\n",
      "            simple -0.281337       16.155989 35.933148      64.066852      0.0       0        129               58    359\n",
      "simple_w_condition -0.174757        7.281553 41.262136      58.737864      0.0       0         85               15    206\n",
      "        comparison -0.165644        1.226994 41.717791      58.282209      0.0       0         68                2    163\n",
      "       aggregation -0.440994        4.968944 27.950311      72.049689      0.0       0         45                8    161\n",
      "               set  0.104000        0.800000 55.200000      44.800000      0.0       0         69                1    125\n",
      "     false_premise  0.647059        0.000000 82.352941      17.647059      0.0       0        126                0    153\n",
      "   post-processing -0.590909        4.545455 20.454545      79.545455      0.0       0          9                2     44\n",
      "         multi-hop  0.048387        4.838710 52.419355      47.580645      0.0       0         65                6    124\n",
      "dict_keys(['domain', 'static_or_dynamic', 'question_type'])\n",
      "static_or_dynamic     Score  Exact Accuracy  Accuracy  Hallucination  Missing  N Miss  N Correct  N Correct Exact  Total\n",
      "           static  0.012048        8.567604 50.602410      49.397590      0.0       0        378               64    747\n",
      "    slow-changing -0.099656        5.841924 45.017182      54.982818      0.0       0        131               17    291\n",
      "    fast-changing -0.299435        2.259887 35.028249      64.971751      0.0       0         62                4    177\n",
      "        real-time -0.700000        5.833333 15.000000      85.000000      0.0       0         18                7    120\n",
      "dict_keys(['domain', 'static_or_dynamic', 'question_type'])\n",
      " domain     Score  Exact Accuracy  Accuracy  Hallucination  Missing  N Miss  N Correct  N Correct Exact  Total\n",
      "finance -0.423313        3.680982 28.834356      71.165644      0.0       0         94               12    326\n",
      "  music  0.039106        2.793296 51.955307      48.044693      0.0       0         93                5    179\n",
      "  movie -0.152104       11.003236 42.394822      57.605178      0.0       0        131               34    309\n",
      " sports -0.075099        4.743083 46.245059      53.754941      0.0       0        117               12    253\n",
      "   open  0.156716       10.820896 57.835821      42.164179      0.0       0        155               29    268\n"
     ]
    }
   ],
   "source": [
    "# GPT-4\n",
    "file_path = '/home/jupyter/cs245-project-crag-master/output/data/modified_rag/gpt-4o-mini/detailed_evaluation_results.json'\n",
    "analyze_query_types(file_path,'question_type')\n",
    "analyze_query_types(file_path,'static_or_dynamic')\n",
    "analyze_query_types(file_path,'domain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6de84ad0-8d9c-4f02-afef-4bcc7ef844bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['domain', 'static_or_dynamic', 'question_type'])\n",
      "     question_type     Score  Exact Accuracy  Accuracy  Hallucination  Missing  N Miss  N Correct  N Correct Exact  Total\n",
      "            simple  0.142061       16.155989 57.103064      42.896936      0.0       0        205               58    359\n",
      "simple_w_condition  0.106796        7.281553 55.339806      44.660194      0.0       0        114               15    206\n",
      "        comparison  0.153374        1.226994 57.668712      42.331288      0.0       0         94                2    163\n",
      "       aggregation -0.068323        4.968944 46.583851      53.416149      0.0       0         75                8    161\n",
      "               set  0.072000        0.800000 53.600000      46.400000      0.0       0         67                1    125\n",
      "     false_premise  0.464052        0.000000 73.202614      26.797386      0.0       0        112                0    153\n",
      "   post-processing  0.045455        4.545455 52.272727      47.727273      0.0       0         23                2     44\n",
      "         multi-hop  0.032258        4.838710 51.612903      48.387097      0.0       0         64                6    124\n",
      "dict_keys(['domain', 'static_or_dynamic', 'question_type'])\n",
      "static_or_dynamic    Score  Exact Accuracy  Accuracy  Hallucination  Missing  N Miss  N Correct  N Correct Exact  Total\n",
      "           static 0.194110        8.567604 59.705489      40.294511      0.0       0        446               64    747\n",
      "    slow-changing 0.044674        5.841924 52.233677      47.766323      0.0       0        152               17    291\n",
      "    fast-changing 0.016949        2.259887 50.847458      49.152542      0.0       0         90                4    177\n",
      "        real-time 0.033333        5.833333 51.666667      48.333333      0.0       0         62                7    120\n",
      "dict_keys(['domain', 'static_or_dynamic', 'question_type'])\n",
      " domain     Score  Exact Accuracy  Accuracy  Hallucination  Missing  N Miss  N Correct  N Correct Exact  Total\n",
      "finance  0.134969        3.680982 56.748466      43.251534      0.0       0        185               12    326\n",
      "  music  0.229050        2.793296 61.452514      38.547486      0.0       0        110                5    179\n",
      "  movie  0.145631       11.003236 57.281553      42.718447      0.0       0        177               34    309\n",
      " sports -0.043478        4.743083 47.826087      52.173913      0.0       0        121               12    253\n",
      "   open  0.223881       10.820896 61.194030      38.805970      0.0       0        164               29    268\n"
     ]
    }
   ],
   "source": [
    "# Llama 3.2 3B\n",
    "file_path = '/home/jupyter/cs245-project-crag-master/output/data/modified_rag/gpt-4o-mini/detailed_evaluation_results.json'\n",
    "analyze_query_types(file_path,'question_type')\n",
    "analyze_query_types(file_path,'static_or_dynamic')\n",
    "analyze_query_types(file_path,'domain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686854d3-1c3d-4d9e-8df6-3e325a57150d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
